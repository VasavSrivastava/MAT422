{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkAm2UdEcz2fmLvUKypUed",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VasavSrivastava/MAT422/blob/main/HW11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3.7.1 Mathematical Formulation**\n",
        "\n",
        "Mathematical formulation in neural networks involves determining the values of nodes in a layer from the preceding layer using weights and biases. For a given layer $l$, the input to the $j'$th node is calculated as:\n",
        "\n",
        "$$\n",
        "z^{(l)}_{j'} = \\sum_{j=1}^{J_{l-1}} w^{(l)}_{j,j'} a^{(l-1)}_j + b^{(l)}_{j'}\n",
        "$$\n",
        "\n",
        "where $w^{(l)}_{j,j'}$ are the weights, $a^{(l-1)}_j$ are the activations from the $(l-1)$th layer, and $b^{(l)}_{j'}$ is the bias. After applying an activation function $\\sigma$, the output of the $j'$th node becomes:\n",
        "\n",
        "$$\n",
        "a^{(l)}_{j'} = \\sigma(z^{(l)}_{j'})\n",
        "$$\n",
        "\n",
        "In matrix form, the relationship for all nodes in layer $l$ is given by:\n",
        "\n",
        "$$\n",
        "z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}\n",
        "$$\n",
        "\n",
        "and the activations are calculated as:\n",
        "\n",
        "$$\n",
        "a^{(l)} = \\sigma(z^{(l)}) = \\sigma(W^{(l)} a^{(l-1)} + b^{(l)})\n",
        "$$\n",
        "\n",
        "Here, $W^{(l)}$ is the weight matrix containing all the weights $w^{(l)}_{j,j'}$, $b^{(l)}$ is the bias vector, and $\\sigma$ is the activation function applied element-wise to $z^{(l)}$.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-uo680WPSzz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define input values (layer l-1)\n",
        "a_prev = np.array([0.5, 0.2, 0.1])  # Example inputs to the network\n",
        "\n",
        "# Define weights and biases for layer l\n",
        "W = np.array([[0.2, 0.8, -0.5],\n",
        "              [0.7, -0.1, 0.9]])  # Weight matrix (2 nodes in layer l)\n",
        "b = np.array([0.1, -0.3])         # Bias vector (2 nodes in layer l)\n",
        "\n",
        "# Define the activation function (sigmoid in this example)\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Compute z (linear combination of weights and inputs)\n",
        "z = np.dot(W, a_prev) + b  # z^(l) = W^(l) * a^(l-1) + b^(l)\n",
        "\n",
        "# Compute activations for layer l\n",
        "a = sigmoid(z)  # a^(l) = Ïƒ(z^(l))\n",
        "\n",
        "# Output results\n",
        "print(\"Input values (a^(l-1)):\", a_prev)\n",
        "print(\"Weights (W^(l)):\\n\", W)\n",
        "print(\"Biases (b^(l)):\", b)\n",
        "print(\"Linear combination (z^(l)):\", z)\n",
        "print(\"Activations (a^(l)):\", a)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSmeEBFMZT54",
        "outputId": "dd0d4507-a5d5-473c-c8ca-5e77e718e2f2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input values (a^(l-1)): [0.5 0.2 0.1]\n",
            "Weights (W^(l)):\n",
            " [[ 0.2  0.8 -0.5]\n",
            " [ 0.7 -0.1  0.9]]\n",
            "Biases (b^(l)): [ 0.1 -0.3]\n",
            "Linear combination (z^(l)): [0.31 0.12]\n",
            "Activations (a^(l)): [0.57688526 0.52996405]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3.7.2 Activation Functions**\n",
        "\n",
        "Activation functions in neural networks abstract the output of a node given an input or set of inputs, enabling specific tasks such as classification. Represented as $\\sigma$, the activation function is applied uniformly across all nodes in a layer:\n",
        "\n",
        "$$\n",
        "a^{(l)} = \\sigma(z^{(l)}) = \\sigma(W^{(l)} a^{(l-1)} + b^{(l)})\n",
        "$$\n",
        "\n",
        "##### **3.7.2.1 Step function**\n",
        "The step function is defined as:\n",
        "\n",
        "$$\n",
        "\\sigma(x) =\n",
        "\\begin{cases}\n",
        "0, & x < 0 \\\\\n",
        "1, & x \\geq 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Also called the Heaviside step function or the unit step function, it often represents a signal that switches on at a specified time and stays switched on indefinitely. It is commonly used for classification problems.\n",
        "\n",
        "##### **3.7.2.2 ReLU function**\n",
        "The ReLU (Rectified Linear Unit) function is defined as:\n",
        "\n",
        "$$\n",
        "\\sigma(x) = \\max(0, x)\n",
        "$$\n",
        "\n",
        "ReLU is one of the most widely used activation functions. It allows signals to either pass through untouched or die completely, enabling faster and more effective training of deep neural architectures, especially for large and complex datasets, compared to sigmoid or similar functions.\n",
        "\n",
        "##### **3.7.2.3 Sigmoid**\n",
        "The sigmoid (or logistic) function is defined as:\n",
        "\n",
        "$$\n",
        "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
        "$$\n",
        "\n",
        "The logistic function is used in various fields, including biomathematics, and can be applied in the output layer of neural networks for predicting probabilities.\n",
        "\n",
        "##### **3.7.2.4 Softmax function**\n",
        "The softmax function converts a vector of numbers into a vector of probabilities, where each probability is proportional to the relative scale of its corresponding value. It is defined as:\n",
        "\n",
        "$$\n",
        "\\sigma(z_k) = \\frac{e^{z_k}}{\\sum_{k=1}^K e^{z_k}}\n",
        "$$\n",
        "\n",
        "Softmax is commonly used in the output layer of neural networks, particularly for multi-class classification problems.\n"
      ],
      "metadata": {
        "id": "MNUnb7GiZZBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step Function\n",
        "def step_function(x):\n",
        "    return np.where(x >= 0, 1, 0)\n",
        "\n",
        "# ReLU Function\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Sigmoid Function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Softmax Function\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))  # For numerical stability\n",
        "    return exp_x / np.sum(exp_x)\n",
        "\n",
        "# Example Inputs\n",
        "x = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
        "print(\"Input:\", x)\n",
        "\n",
        "# Step Function Output\n",
        "step_output = step_function(x)\n",
        "print(\"\\nStep Function Output:\", step_output)\n",
        "\n",
        "# ReLU Function Output\n",
        "relu_output = relu(x)\n",
        "print(\"\\nReLU Function Output:\", relu_output)\n",
        "\n",
        "# Sigmoid Function Output\n",
        "sigmoid_output = sigmoid(x)\n",
        "print(\"\\nSigmoid Function Output:\", sigmoid_output)\n",
        "\n",
        "# Softmax Function Output\n",
        "softmax_output = softmax(x)\n",
        "print(\"\\nSoftmax Function Output:\", softmax_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nP_2F7Kc_B-",
        "outputId": "6c62aa5c-70e0-4abe-a3d6-b8770378dfc2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [-2. -1.  0.  1.  2.]\n",
            "\n",
            "Step Function Output: [0 0 1 1 1]\n",
            "\n",
            "ReLU Function Output: [0. 0. 0. 1. 2.]\n",
            "\n",
            "Sigmoid Function Output: [0.11920292 0.26894142 0.5        0.73105858 0.88079708]\n",
            "\n",
            "Softmax Function Output: [0.01165623 0.03168492 0.08612854 0.23412166 0.63640865]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3.7.3 Cost Function**\n",
        "\n",
        "The cost function in neural networks measures the difference between the predicted outputs ($\\hat{y}$) and the actual outputs ($y$) from the training data. For regression tasks, the least squares cost function is commonly used and is defined as:\n",
        "\n",
        "$$\n",
        "J = \\frac{1}{2} \\sum_{n=1}^{N} \\sum_{k=1}^{K} \\left( \\hat{y}^{(n)}_k - y^{(n)}_k \\right)^2\n",
        "$$\n",
        "\n",
        "where $N$ is the number of training examples, and $K$ is the number of output nodes. For classification problems, the cost function is often based on logistic regression. For binary classification ($y^{(n)} \\in \\{0, 1\\}$), the cost function is:\n",
        "\n",
        "$$\n",
        "J = -\\sum_{n=1}^{N} \\left( y^{(n)} \\ln \\hat{y}^{(n)} + (1 - y^{(n)}) \\ln (1 - \\hat{y}^{(n)}) \\right)\n",
        "$$\n",
        "\n",
        "This is also known as the cross-entropy loss, which is widely used in classification tasks for its effectiveness in optimizing probabilities.\n"
      ],
      "metadata": {
        "id": "qVh70MIndBDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example outputs and predictions\n",
        "y_actual_regression = np.array([1.5, 2.0, 3.0])  # Actual values for regression\n",
        "y_predicted_regression = np.array([1.4, 2.1, 2.9])  # Predicted values for regression\n",
        "\n",
        "y_actual_classification = np.array([1, 0, 1])  # Actual values for classification (binary)\n",
        "y_predicted_classification = np.array([0.9, 0.2, 0.8])  # Predicted probabilities for classification\n",
        "\n",
        "# Mean Squared Error (MSE) Cost Function\n",
        "def mean_squared_error(y_actual, y_predicted):\n",
        "    return np.mean((y_actual - y_predicted) ** 2)\n",
        "\n",
        "# Cross-Entropy Loss for Binary Classification\n",
        "def cross_entropy_loss(y_actual, y_predicted):\n",
        "    epsilon = 1e-15  # To prevent log(0)\n",
        "    y_predicted = np.clip(y_predicted, epsilon, 1 - epsilon)  # Clamp predicted values\n",
        "    return -np.mean(y_actual * np.log(y_predicted) + (1 - y_actual) * np.log(1 - y_predicted))\n",
        "\n",
        "# Calculate the losses\n",
        "mse_loss = mean_squared_error(y_actual_regression, y_predicted_regression)\n",
        "cross_entropy_loss_value = cross_entropy_loss(y_actual_classification, y_predicted_classification)\n",
        "\n",
        "# Print the results\n",
        "print(\"Mean Squared Error (MSE) Loss:\", mse_loss)\n",
        "print(\"Cross-Entropy Loss:\", cross_entropy_loss_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cAOHmtbgyoq",
        "outputId": "a6888f26-66d3-4dab-9e66-90f553d30b42"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE) Loss: 0.010000000000000018\n",
            "Cross-Entropy Loss: 0.18388253942874858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3.7.4 Backpropagation**\n",
        "\n",
        "Backpropagation is the core mechanism for training neural networks by fine-tuning the weights ($W$) and biases ($b$) to minimize the cost function $J$. It calculates the gradient of the cost function with respect to each parameter using the chain rule. For a layer $l$, backpropagation introduces $\\delta^{(l)}_{j'} = \\frac{\\partial J}{\\partial z^{(l)}_{j'}}$, which propagates the error backward through the network. The weight gradients are computed as:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial w^{(l)}_{j,j'}} = \\delta^{(l)}_{j'} a^{(l-1)}_j\n",
        "$$\n",
        "\n",
        "and the bias gradients as:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial b^{(l)}_{j'}} = \\delta^{(l)}_{j'}.\n",
        "$$\n",
        "\n",
        "The error term $\\delta$ depends on the activation function used. For ReLU, the derivative is either $0$ or $1$, and for the logistic function, it is $\\sigma(z)(1 - \\sigma(z))$. Backpropagation allows gradients to propagate backward from the output layer to earlier layers, enabling efficient optimization via gradient descent.\n"
      ],
      "metadata": {
        "id": "g117meaQhMLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the sigmoid activation function and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# Input data (X) and corresponding labels (y)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input: XOR problem\n",
        "y = np.array([[0], [1], [1], [0]])  # Output: XOR labels\n",
        "\n",
        "# Initialize weights and biases\n",
        "np.random.seed(42)\n",
        "W1 = np.random.rand(2, 2)  # Weights for input to hidden layer\n",
        "b1 = np.random.rand(1, 2)  # Biases for hidden layer\n",
        "W2 = np.random.rand(2, 1)  # Weights for hidden to output layer\n",
        "b2 = np.random.rand(1, 1)  # Biases for output layer\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Train the network\n",
        "epochs = 10000\n",
        "for epoch in range(epochs):\n",
        "    # Feedforward\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = sigmoid(z1)  # Activation of hidden layer\n",
        "\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)  # Output layer predictions\n",
        "\n",
        "    # Compute the loss (mean squared error)\n",
        "    loss = np.mean((y - a2) ** 2)\n",
        "\n",
        "    # Backpropagation\n",
        "    d_a2 = a2 - y  # Derivative of loss w.r.t. output activations\n",
        "    d_z2 = d_a2 * sigmoid_derivative(z2)  # Derivative of loss w.r.t. z2\n",
        "    d_W2 = np.dot(a1.T, d_z2)  # Derivative of loss w.r.t. W2\n",
        "    d_b2 = np.sum(d_z2, axis=0, keepdims=True)  # Derivative of loss w.r.t. b2\n",
        "\n",
        "    d_a1 = np.dot(d_z2, W2.T)  # Backpropagating error to hidden layer\n",
        "    d_z1 = d_a1 * sigmoid_derivative(z1)  # Derivative of loss w.r.t. z1\n",
        "    d_W1 = np.dot(X.T, d_z1)  # Derivative of loss w.r.t. W1\n",
        "    d_b1 = np.sum(d_z1, axis=0, keepdims=True)  # Derivative of loss w.r.t. b1\n",
        "\n",
        "    # Update weights and biases\n",
        "    W2 -= learning_rate * d_W2\n",
        "    b2 -= learning_rate * d_b2\n",
        "    W1 -= learning_rate * d_W1\n",
        "    b1 -= learning_rate * d_b1\n",
        "\n",
        "    # Print loss every 1000 epochs\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Final predictions\n",
        "print(\"\\nFinal Predictions:\")\n",
        "print(a2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsFX6CBfhUnK",
        "outputId": "1ab68ea0-071f-4c66-905f-5de8c99207e3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.3247\n",
            "Epoch 1000, Loss: 0.2406\n",
            "Epoch 2000, Loss: 0.1960\n",
            "Epoch 3000, Loss: 0.1207\n",
            "Epoch 4000, Loss: 0.0305\n",
            "Epoch 5000, Loss: 0.0125\n",
            "Epoch 6000, Loss: 0.0074\n",
            "Epoch 7000, Loss: 0.0051\n",
            "Epoch 8000, Loss: 0.0038\n",
            "Epoch 9000, Loss: 0.0031\n",
            "\n",
            "Final Predictions:\n",
            "[[0.05322146]\n",
            " [0.95171535]\n",
            " [0.95160449]\n",
            " [0.05175396]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3.7.5 Backpropagation Algorithm**\n",
        "\n",
        "The backpropagation algorithm is a systematic method for training neural networks by iteratively updating weights and biases to minimize the cost function. It begins by initializing weights and biases randomly, followed by feeding input data through the network to compute activations ($a$) and outputs ($\\hat{y}$). The gradient of the cost function is calculated using the chain rule, starting with the output layer error:\n",
        "\n",
        "$$\n",
        "\\delta^{(L)}_j = \\frac{d\\sigma}{dz} \\bigg|_{z^{(L)}_j} (\\hat{y} - y).\n",
        "$$\n",
        "\n",
        "Errors are propagated backward through the layers:\n",
        "\n",
        "$$\n",
        "\\delta^{(l-1)}_j = \\frac{d\\sigma}{dz} \\bigg|_{z^{(l-1)}_j} \\sum_j \\delta^{(l)}_{j'} w^{(l)}_{j,j'}.\n",
        "$$\n",
        "\n",
        "Weights and biases are then updated using gradient descent:\n",
        "\n",
        "$$\n",
        "w^{(l)}_{j,j'} \\gets w^{(l)}_{j,j'} - \\beta \\delta^{(l)}_{j'} a^{(l-1)}_j,\n",
        "$$\n",
        "\n",
        "$$\n",
        "b^{(l)}_{j'} \\gets b^{(l)}_{j'} - \\beta \\delta^{(l)}_{j'}.\n",
        "$$\n",
        "\n",
        "This process repeats until the desired accuracy is achieved.\n"
      ],
      "metadata": {
        "id": "33zWMtQchsbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define activation function (sigmoid) and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)  # Assumes input is already passed through sigmoid\n",
        "\n",
        "# Input data (X) and target output (y)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # XOR input\n",
        "y = np.array([[0], [1], [1], [0]])  # XOR output\n",
        "\n",
        "# Initialize weights and biases\n",
        "np.random.seed(42)\n",
        "W1 = np.random.rand(2, 2)  # Weights for input to hidden layer\n",
        "b1 = np.random.rand(1, 2)  # Biases for hidden layer\n",
        "W2 = np.random.rand(2, 1)  # Weights for hidden to output layer\n",
        "b2 = np.random.rand(1, 1)  # Biases for output layer\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Training loop\n",
        "epochs = 10000\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = sigmoid(z1)  # Hidden layer activations\n",
        "\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)  # Output layer activations (predictions)\n",
        "\n",
        "    # Compute the loss (Mean Squared Error)\n",
        "    loss = np.mean((y - a2) ** 2)\n",
        "\n",
        "    # Backward pass\n",
        "    # Output layer error and delta\n",
        "    error_output = a2 - y\n",
        "    delta_output = error_output * sigmoid_derivative(a2)\n",
        "\n",
        "    # Hidden layer error and delta\n",
        "    error_hidden = np.dot(delta_output, W2.T)\n",
        "    delta_hidden = error_hidden * sigmoid_derivative(a1)\n",
        "\n",
        "    # Gradient calculation\n",
        "    d_W2 = np.dot(a1.T, delta_output)\n",
        "    d_b2 = np.sum(delta_output, axis=0, keepdims=True)\n",
        "    d_W1 = np.dot(X.T, delta_hidden)\n",
        "    d_b1 = np.sum(delta_hidden, axis=0, keepdims=True)\n",
        "\n",
        "    # Update weights and biases\n",
        "    W2 -= learning_rate * d_W2\n",
        "    b2 -= learning_rate * d_b2\n",
        "    W1 -= learning_rate * d_W1\n",
        "    b1 -= learning_rate * d_b1\n",
        "\n",
        "    # Print loss every 1000 epochs\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Final predictions\n",
        "print(\"\\nFinal Predictions:\")\n",
        "print(a2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImD24a98ixNM",
        "outputId": "8b83ee00-5add-44f8-fb8d-6f6da8d5b4ad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.3247\n",
            "Epoch 1000, Loss: 0.2406\n",
            "Epoch 2000, Loss: 0.1960\n",
            "Epoch 3000, Loss: 0.1207\n",
            "Epoch 4000, Loss: 0.0305\n",
            "Epoch 5000, Loss: 0.0125\n",
            "Epoch 6000, Loss: 0.0074\n",
            "Epoch 7000, Loss: 0.0051\n",
            "Epoch 8000, Loss: 0.0038\n",
            "Epoch 9000, Loss: 0.0031\n",
            "\n",
            "Final Predictions:\n",
            "[[0.05322146]\n",
            " [0.95171535]\n",
            " [0.95160449]\n",
            " [0.05175396]]\n"
          ]
        }
      ]
    }
  ]
}