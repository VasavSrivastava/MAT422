{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOG8Lz+EzYGFHfF0L2MZW1W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VasavSrivastava/MAT422/blob/main/HW10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3.5 K-means**\n",
        "\n",
        "K-means is an unsupervised clustering algorithm that partitions a dataset into $k$ clusters, where each data point belongs to the cluster with the nearest mean, serving as a prototype of the cluster. Given a set of $n$ data points $X = {x_1, x_2, \\dots, x_n}$ in $d$-dimensional space, the algorithm iteratively assigns each point $x_i$ to one of $k$ clusters $C_1, C_2, \\dots, C_k$ by minimizing the within-cluster sum of squares. The objective function to minimize is:\n",
        "\n",
        "$ J = \\sum_{j=1}^{k} \\sum_{x_i \\in C_j} | x_i - \\mu_j |^2 $\n",
        "\n",
        "where $\\mu_j$ is the centroid of cluster $C_j$ and $| x_i - \\mu_j |^2$ represents the squared Euclidean distance between a data point $x_i$ and the cluster centroid $\\mu_j$. The centroids $\\mu_j$ are updated after each iteration as the mean of all points assigned to $C_j$:\n",
        "\n",
        "$ \\mu_j = \\frac{1}{|C_j|} \\sum_{x_i \\in C_j} x_i $\n",
        "\n",
        "The algorithm stops when cluster assignments no longer change or the decrease in $J$ falls below a threshold.\n",
        "\n"
      ],
      "metadata": {
        "id": "-qATQq1ubpSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generate some random data points\n",
        "np.random.seed(42)\n",
        "data = np.random.rand(100, 2)  # 100 points in 2D space\n",
        "\n",
        "# K-means parameters\n",
        "k = 3  # number of clusters\n",
        "n_iterations = 100  # maximum number of iterations\n",
        "\n",
        "# Step 1: Initialize centroids randomly from the data points\n",
        "centroids = data[np.random.choice(data.shape[0], k, replace=False)]\n",
        "\n",
        "# Function to calculate the Euclidean distance\n",
        "def euclidean_distance(a, b):\n",
        "    return np.sqrt(np.sum((a - b) ** 2))\n",
        "\n",
        "# Step 2: K-means algorithm\n",
        "for _ in range(n_iterations):\n",
        "    # Assign each point to the nearest centroid\n",
        "    clusters = [[] for _ in range(k)]\n",
        "    for point in data:\n",
        "        distances = [euclidean_distance(point, centroid) for centroid in centroids]\n",
        "        closest_centroid = np.argmin(distances)\n",
        "        clusters[closest_centroid].append(point)\n",
        "\n",
        "    # Step 3: Update centroids\n",
        "    new_centroids = np.array([np.mean(cluster, axis=0) if cluster else centroids[i] for i, cluster in enumerate(clusters)])\n",
        "\n",
        "    # Check for convergence (if centroids do not change)\n",
        "    if np.all(centroids == new_centroids):\n",
        "        break\n",
        "    centroids = new_centroids\n",
        "\n",
        "# Display the final centroids and clusters\n",
        "print(\"Final centroids:\\n\", centroids)\n",
        "for i, cluster in enumerate(clusters):\n",
        "    print(f\"Cluster {i+1}: {len(cluster)} points\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pxpnx9QmhdF-",
        "outputId": "7ec06770-dc33-416f-a879-d05cb2e71b90"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final centroids:\n",
            " [[0.8039633  0.57026999]\n",
            " [0.18520943 0.72228065]\n",
            " [0.36376248 0.20008043]]\n",
            "Cluster 1: 38 points\n",
            "Cluster 2: 28 points\n",
            "Cluster 3: 34 points\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3.6 Support Vector Machine**\n",
        "\n",
        "Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression tasks, designed to find the optimal hyperplane that maximizes the margin between different classes in a dataset. Given a set of training points ${(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)}$ where $x_i$ is a feature vector and $y_i \\in {-1, 1}$ represents the class labels, SVM aims to find a hyperplane defined by $w \\cdot x + b = 0$ that maximizes the distance (margin) between the nearest points of the classes. The optimization problem can be formulated as:\n",
        "\n",
        "$ \\min_{w, b} \\frac{1}{2} |w|^2 $\n",
        "\n",
        "subject to the constraint:\n",
        "\n",
        "$ y_i (w \\cdot x_i + b) \\geq 1, \\quad \\forall i = 1, 2, \\dots, n $\n",
        "\n",
        "where $w$ is the weight vector perpendicular to the hyperplane and $b$ is the bias term. This objective function minimizes the norm of $w$, effectively maximizing the margin. If the data is not linearly separable, SVM introduces a slack variable $\\xi_i$ and a regularization parameter $C$ to allow some misclassification, modifying the constraint to:\n",
        "\n",
        "$ y_i (w \\cdot x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0 $\n",
        "\n",
        "The final objective function becomes:\n",
        "\n",
        "$ \\min_{w, b} \\frac{1}{2} |w|^2 + C \\sum_{i=1}^{n} \\xi_i $\n",
        "\n",
        "where $C$ controls the trade-off between maximizing the margin and minimizing classification errors."
      ],
      "metadata": {
        "id": "0pu7dN1rhhVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Load the iris dataset and select only two classes for binary classification\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Filter the dataset to only two classes for binary classification\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Support Vector Classifier with a linear kernel\n",
        "svc = SVC(kernel='linear', C=1.0)\n",
        "\n",
        "# Train the SVM model\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svc.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy and classification report\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9U7hlRkmGGn",
        "outputId": "a501482e-fc77-4a11-cf0e-95232f9ab425"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        17\n",
            "           1       1.00      1.00      1.00        13\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        }
      ]
    }
  ]
}