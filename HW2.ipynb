{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlTi73Clbzw0adb0jjVn+g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VasavSrivastava/MAT422/blob/main/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1.3.1 QR Decomposition**\n",
        "\n",
        "#####QR decomposition is a matrix factorization technique where a matrix A is decomposed into the product of two matrices: $Q$ and $R$. Specifically, for a $m$ x $n$ matrix $A$, QR decomposition expresses $A$ as: $$A = QR$$\n",
        "\n",
        "#####Where $Q$ is an orthogonal matrix, meaning that its columns are orthonormal vectors and $R$ is an upper triangular matrix, i.e every element below the diagonal are zero.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7dzcuLLB12Yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a square matrix A\n",
        "A = np.array([[2, -2, 18],\n",
        "              [2, 1, 0],\n",
        "              [1, 2, 0]])\n",
        "\n",
        "# Perform QR decomposition\n",
        "Q, R = np.linalg.qr(A)\n",
        "\n",
        "# Output the results\n",
        "print(\"Matrix A:\")\n",
        "print(A)\n",
        "print(\"\\nMatrix Q (Orthogonal Matrix):\")\n",
        "print(Q)\n",
        "print(\"\\nMatrix R (Upper Triangular Matrix):\")\n",
        "print(R)\n",
        "\n",
        "# Verify the decomposition (A should be equal to Q @ R)\n",
        "A_reconstructed = np.dot(Q, R)\n",
        "print(\"\\nReconstructed Matrix A (Q @ R):\")\n",
        "print(A_reconstructed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rDKJRUV-CGi",
        "outputId": "7bd933c2-fcef-4e1f-f448-c92b1889b21d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix A:\n",
            "[[ 2 -2 18]\n",
            " [ 2  1  0]\n",
            " [ 1  2  0]]\n",
            "\n",
            "Matrix Q (Orthogonal Matrix):\n",
            "[[-0.66666667  0.66666667  0.33333333]\n",
            " [-0.66666667 -0.33333333 -0.66666667]\n",
            " [-0.33333333 -0.66666667  0.66666667]]\n",
            "\n",
            "Matrix R (Upper Triangular Matrix):\n",
            "[[-3.0000000e+00  4.4408921e-16 -1.2000000e+01]\n",
            " [ 0.0000000e+00 -3.0000000e+00  1.2000000e+01]\n",
            " [ 0.0000000e+00  0.0000000e+00  6.0000000e+00]]\n",
            "\n",
            "Reconstructed Matrix A (Q @ R):\n",
            "[[ 2.00000000e+00 -2.00000000e+00  1.80000000e+01]\n",
            " [ 2.00000000e+00  1.00000000e+00  2.22044605e-16]\n",
            " [ 1.00000000e+00  2.00000000e+00  2.22044605e-16]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1.3.2 Least-squares problems**\n",
        "\n",
        "#####Least squares problems are problems where the goal is to find the best-fit solution to an overdetermined system of linear equations (i.e., there are more equations than unknowns). The idea is to minimize the sum of the squared differences between the observed data points and the model's predicted values. Given $A \\in \\mathbb{R}^{m x n}$ and $\\textbf{b} \\in \\mathbb{R}^m$. The least squares approach looks for $\\textbf{x}$ that minimizes the residual sum of squares: $$\\underset{x}{min}\\:||A\\textbf{x}-\\textbf{b}||^2$$"
      ],
      "metadata": {
        "id": "LxT5o-MBAMF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sample data points (x, y)\n",
        "x = np.array([0, 1, 2, 3, 4])\n",
        "y = np.array([1, 3, 2, 5, 7])\n",
        "\n",
        "# Construct the design matrix A (add a column of ones for the intercept)\n",
        "A = np.vstack([x, np.ones(len(x))]).T\n",
        "\n",
        "# Solve the least squares problem: find m (slope) and c (intercept) such that y = mx + c\n",
        "m, c = np.linalg.lstsq(A, y, rcond=None)[0]\n",
        "\n",
        "# Print the slope and intercept\n",
        "print(f\"Slope (m): {m}\")\n",
        "print(f\"Intercept (c): {c}\")\n",
        "\n",
        "# Generate the line of best fit\n",
        "line = m * x + c\n",
        "\n",
        "# Output the fitted line values\n",
        "print(\"\\nFitted line values:\")\n",
        "print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShjIBFxiNgF9",
        "outputId": "c59d9d8b-fdc8-4fbf-f05f-764ec715c6ea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slope (m): 1.4\n",
            "Intercept (c): 0.7999999999999989\n",
            "\n",
            "Fitted line values:\n",
            "[0.8 2.2 3.6 5.  6.4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1.3.3 Linear Regression**\n",
        "\n",
        "#####Linear regression is used to model the linear relationship between a dependent variable $y$ and one or more independent variables $x_1, x_2,...., x_n$. The goal is to find the best-fitting line or hyperplane that predicts $y$ based on the input variables. The formula for simple linear regression with one independent variable is $y=mx+c$, where $m$ is the slope and $c$ is the intercept."
      ],
      "metadata": {
        "id": "N4_2ybcBN7y2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data: x represents the independent variable and y the dependent variable\n",
        "x = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Reshape because scikit-learn expects 2D input\n",
        "y = np.array([1, 4, 9, 16, 25])  # This is a quadratic relationship, but we'll fit a linear model for simplicity\n",
        "\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model to the data\n",
        "model.fit(x, y)\n",
        "\n",
        "# Print the slope (m) and intercept (c)\n",
        "print(f\"Slope (m): {model.coef_[0]}\")\n",
        "print(f\"Intercept (c): {model.intercept_}\")\n",
        "\n",
        "# Predict values based on the model\n",
        "predicted_y = model.predict(x)\n",
        "\n",
        "# Output the predicted values\n",
        "print(\"\\nPredicted values:\")\n",
        "print(predicted_y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxDDhEeIQjdu",
        "outputId": "f06e85e9-25d6-4ba9-e0ec-d0a2232060e8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slope (m): 5.999999999999999\n",
            "Intercept (c): -6.9999999999999964\n",
            "\n",
            "Predicted values:\n",
            "[-1.  5. 11. 17. 23.]\n"
          ]
        }
      ]
    }
  ]
}